This document is a self-made Lab guide based off of the CKA Curriculum v1.8.0

# Application Lifecycle Management - 8%

## Deployments
Reference from kubernetes.io - [Deployments](https://kubernetes.io/docs/concepts/workloads/controllers/deployment/)

A Deployment controller provides declarative updates for Pods and ReplicaSets.

You describe a desired state in a Deployment object, and the Deployment controller changes the actual state to the desired state at a controlled rate. You can define Deployments to create new ReplicaSets, or to remove existing Deployments and adopt all their resources with new Deployments.

## Know various ways to configure applications

Common use cases for Deployments that we will review:
- Create a Deployment to rollout a ReplicaSet
- Updating a Deployment - Declare a new state 
- Rollback to an earlier Deployment revision
- Scale up the Deployment to facilitate more load
- Pause the Deployment to apply fixes
- Use the status of the Deployment as an indicator that a rollout has stuck
- Clean up older ReplicaSets that you dont need anymore

### Create a Deployment to rollout a ReplicaSet

The following is an example of a Deployment. It creates a ReplicaSet to bring up three nginx Pods. For our example we will name this `nginx-deployment.yaml`
```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
  labels:
    app: nginx
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.7.9
        ports:
        - containerPort: 80
```

Details about this example app definition:
- A Deployment named nginx-deployment is created, indicated by the .metadata.name field.
- The Deployment creates three replicated Pods, indicated by the replicas field.
- The selector field defines how the Deployment finds which Pods to manage. In this case, we simply select on one label defined in the Pod template (app: nginx). However, more sophisticated selection rules are possible, as long as the Pod template itself satisfies the rule.
- The Pod template’s specification, or .template.spec field, indicates that the Pods run one container, nginx, which runs the nginx Docker Hub image at version 1.7.9.
- The Deployment opens port 80 for use by the Pods.

Deploy the container:
```
$ kubectl create -f nginx-deployment.yaml
deployment.apps "nginx-deployment" created

$ kubectl rollout status deployment/nginx-deployment
deployment "nginx-deployment" successfully rolled out

$ kubectl get deployments
NAME               DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
nginx-deployment   3         3         3            3           28s
```

Description of fields:
- NAME lists the names of the Deployments in the cluster.
- DESIRED displays the desired number of replicas of the application, which you define when you create the Deployment. This is the desired state.
- CURRENT displays how many replicas are currently running.
- UP-TO-DATE displays the number of replicas that have been updated to achieve the desired state.
- AVAILABLE displays how many replicas of the application are available to your users.
- AGE displays the amount of time that the application has been running.

To see the ReplicaSets created by the deployment run
```
$ kubectl get rs
NAME                          DESIRED   CURRENT   READY     AGE
nginx-deployment-75675f5897   3         3         3         47s
```

To see the labels automatically generated for each pod run:
```
$ kubectl get pods --show-labels
NAME                                READY     STATUS    RESTARTS   AGE       LABELS
counter                             3/3       Running   0          54m       <none>
nginx-deployment-75675f5897-fw8tr   1/1       Running   0          1m        app=nginx,pod-template-hash=3123191453
nginx-deployment-75675f5897-j8zjs   1/1       Running   0          1m        app=nginx,pod-template-hash=3123191453
nginx-deployment-75675f5897-zlb92   1/1       Running   0          1m        app=nginx,pod-template-hash=3123191453
```

The pod-template-hash label is added by the Deployment controller to every ReplicaSet that a Deployment creates or adopts.

This label ensures that child ReplicaSets of a Deployment do not overlap. It is generated by hashing the PodTemplate of the ReplicaSet and using the resulting hash as the label value that is added to the ReplicaSet selector, Pod template labels, and in any existing Pods that the ReplicaSet might have.

### Updating a Deployment 
Following the last example, suppose that we now want to update the nginx Pods to use the nginx:1.9.1 image instead of the nginx:1.7.9 image:
```
$ kubectl set image deployment/nginx-deployment nginx=nginx:1.9.1
deployment.apps "nginx-deployment" image updated

$ kubectl describe deployment nginx-deployment
Name:                   nginx-deployment
Namespace:              default
CreationTimestamp:      Mon, 16 Jul 2018 16:43:16 -0700
Labels:                 app=nginx
Annotations:            deployment.kubernetes.io/revision=2
Selector:               app=nginx
Replicas:               3 desired | 3 updated | 3 total | 3 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:  app=nginx
  Containers:
   nginx:
    Image:        nginx:1.9.1
    Port:         80/TCP
    Host Port:    0/TCP
    Environment:  <none>
    Mounts:       <none>
  Volumes:        <none>
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      True    MinimumReplicasAvailable
  Progressing    True    NewReplicaSetAvailable
OldReplicaSets:  <none>
NewReplicaSet:   nginx-deployment-c4747d96c (3/3 replicas created)
Events:
  Type    Reason             Age   From                   Message
  ----    ------             ----  ----                   -------
  Normal  ScalingReplicaSet  7m    deployment-controller  Scaled up replica set nginx-deployment-75675f5897 to 3
  Normal  ScalingReplicaSet  38s   deployment-controller  Scaled up replica set nginx-deployment-c4747d96c to 1
  Normal  ScalingReplicaSet  29s   deployment-controller  Scaled down replica set nginx-deployment-75675f5897 to 2
  Normal  ScalingReplicaSet  29s   deployment-controller  Scaled up replica set nginx-deployment-c4747d96c to 2
  Normal  ScalingReplicaSet  20s   deployment-controller  Scaled down replica set nginx-deployment-75675f5897 to 1
  Normal  ScalingReplicaSet  20s   deployment-controller  Scaled up replica set nginx-deployment-c4747d96c to 3
  Normal  ScalingReplicaSet  18s   deployment-controller  Scaled down replica set nginx-deployment-75675f5897 to 0

$ kubectl get rs
NAME                          DESIRED   CURRENT   READY     AGE
nginx-deployment-75675f5897   0         0         0         9m
nginx-deployment-c4747d96c    3         3         3         3m

Note: As you can see above, when changing the deployment K8s scaled down the old ReplicaSet to 0 and created a new ReplicaSet
```

Alternatively you can also `edit` the deployment and change `.spec.template.spec.containers[0].image` from `nginx:1.7.9` to `nginx:1.9.1`:
```
$ kubectl edit deployment/nginx-deployment
```

### Rolling Back a Deployment
Sometimes you may want to rollback a Deployment; for example, when the Deployment is not stable, such as crash looping. By default, all of the Deployment’s rollout history is kept in the system so that you can rollback anytime you want (you can change that by modifying revision history limit).

Suppose that we made a typo while updating the Deployment, by putting the image name as nginx:1.91 instead of nginx:1.9.1:
```
$ kubectl set image deployment/nginx-deployment nginx=nginx:1.91
deployment.apps "nginx-deployment" image updated
```

The rollout will be stuck:
```
$ kubectl rollout status deployments nginx-deployment
Waiting for rollout to finish: 1 out of 3 new replicas have been updated...

$ kubectl get deployments
NAME               DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
nginx-deployment   3         4         1            3           14m

$ kubectl get pods
NAME                                READY     STATUS         RESTARTS   AGE
nginx-deployment-595696685f-tskf9   0/1       ErrImagePull   0          1m
nginx-deployment-c4747d96c-qhdn5    1/1       Running        0          8m
nginx-deployment-c4747d96c-rwnv9    1/1       Running        0          9m
nginx-deployment-c4747d96c-sxnlf    1/1       Running        0          8m

$ kubectl get rs
NAME                          DESIRED   CURRENT   READY     AGE
nginx-deployment-595696685f   1         1         0         2m
nginx-deployment-75675f5897   0         0         0         16m
nginx-deployment-c4747d96c    3         3         3         9m
```

To rollback to the previous revision:
```
$ kubectl rollout undo deployment/nginx-deployment
deployment.apps "nginx-deployment"

$ kubectl get deployments
NAME               DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
nginx-deployment   3         3         3            3           17m

$ kubectl get pods
NAME                               READY     STATUS    RESTARTS   AGE
nginx-deployment-c4747d96c-qhdn5   1/1       Running   0          11m
nginx-deployment-c4747d96c-rwnv9   1/1       Running   0          11m
nginx-deployment-c4747d96c-sxnlf   1/1       Running   0          11m

$ kubectl get rs
NAME                          DESIRED   CURRENT   READY     AGE
nginx-deployment-595696685f   0         0         0         4m
nginx-deployment-75675f5897   0         0         0         18m
nginx-deployment-c4747d96c    3         3         3         11m
```

Alternatively you can roll back to a specific revision by adding the `--to-revision` flag:
```
$ kubectl rollout undo deployment/nginx-deployment --to-revision=2
deployment "nginx-deployment" rolled back
```

### Scaling a Deployment
You can scale a Deployment by using the following command:
```
$ kubectl scale deployment nginx-deployment --replicas=10
deployment.extensions "nginx-deployment" scaled

$ kubectl get deployment nginx-deployment
NAME               DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
nginx-deployment   10        10        10           10          21m

$ kubectl get pods
NAME                               READY     STATUS    RESTARTS   AGE
nginx-deployment-c4747d96c-cbzv7   1/1       Running   0          50s
nginx-deployment-c4747d96c-hkm2j   1/1       Running   0          50s
nginx-deployment-c4747d96c-msmrc   1/1       Running   0          50s
nginx-deployment-c4747d96c-p7vlh   1/1       Running   0          50s
nginx-deployment-c4747d96c-qhdn5   1/1       Running   0          14m
nginx-deployment-c4747d96c-qppzs   1/1       Running   0          50s
nginx-deployment-c4747d96c-rwnv9   1/1       Running   0          14m
nginx-deployment-c4747d96c-swwhn   1/1       Running   0          50s
nginx-deployment-c4747d96c-sxnlf   1/1       Running   0          14m
nginx-deployment-c4747d96c-xk8sp   1/1       Running   0          50s
```

### Pausing and Resuming a Deployment
You can pause a Deployment before triggering one or more updates and then resume it. This will allow you to apply multiple fixes in between pausing and resuming without triggering unnecessary rollouts.

For Example:
```
$ kubectl rollout pause deployment/nginx-deployment
deployment.apps "nginx-deployment" paused

Let's update to nginx 1.8
$ kubectl set image deploy/nginx-deployment nginx=nginx:1.8
deployment.apps "nginx-deployment" image updated

Notice that no new rollout started
$ kubectl get deployments
NAME               DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
nginx-deployment   10        10        0            10          24m
```

Resume the Deployment and observe:
```
$ kubectl rollout resume deploy/nginx-deployment
deployment.apps "nginx-deployment" resumed

$ kubectl get deployments
NAME               DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
nginx-deployment   10        10        10           10          26m

$ kubectl get rs
NAME                          DESIRED   CURRENT   READY     AGE
nginx-deployment-595696685f   0         0         0         14m
nginx-deployment-6ddf4c5bf7   10        10        10        1m
nginx-deployment-75675f5897   0         0         0         27m
nginx-deployment-c4747d96c    0         0         0         21m
```

### Deployment Status
A Deployment enters various states during its lifecycle. It can be progressing while rolling out a new ReplicaSet, it can be complete, or it can fail to progress.

You can check if a Deployment has completed by using kubectl rollout status. If the rollout completed successfully, kubectl rollout status returns a zero exit code.
```
$ kubectl rollout status deploy/nginx-deployment
Waiting for rollout to finish: 2 of 3 updated replicas are available...
deployment "nginx" successfully rolled out
$ echo $?
0
```

Other useful commands:
```
$ kubectl describe deployment nginx-deployment
Name:                   nginx-deployment
Namespace:              default
CreationTimestamp:      Mon, 16 Jul 2018 16:43:16 -0700
Labels:                 app=nginx
Annotations:            deployment.kubernetes.io/revision=5
Selector:               app=nginx
Replicas:               10 desired | 10 updated | 10 total | 10 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:  app=nginx
  Containers:
   nginx:
    Image:        nginx:1.8
    Port:         80/TCP
    Host Port:    0/TCP
    Environment:  <none>
    Mounts:       <none>
  Volumes:        <none>
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      True    MinimumReplicasAvailable
  Progressing    True    NewReplicaSetAvailable
OldReplicaSets:  <none>
NewReplicaSet:   nginx-deployment-6ddf4c5bf7 (10/10 replicas created)
Events:
  Type    Reason              Age                From                   Message
  ----    ------              ----               ----                   -------
  Normal  ScalingReplicaSet   32m                deployment-controller  Scaled up replica set nginx-deployment-75675f5897 to 3
  Normal  ScalingReplicaSet   26m                deployment-controller  Scaled up replica set nginx-deployment-c4747d96c to 1
  Normal  ScalingReplicaSet   25m                deployment-controller  Scaled down replica set nginx-deployment-75675f5897 to 2
  Normal  ScalingReplicaSet   25m                deployment-controller  Scaled up replica set nginx-deployment-c4747d96c to 2
  Normal  ScalingReplicaSet   25m                deployment-controller  Scaled down replica set nginx-deployment-75675f5897 to 1
  Normal  ScalingReplicaSet   25m                deployment-controller  Scaled up replica set nginx-deployment-c4747d96c to 3
  Normal  ScalingReplicaSet   25m                deployment-controller  Scaled down replica set nginx-deployment-75675f5897 to 0
  Normal  ScalingReplicaSet   18m                deployment-controller  Scaled up replica set nginx-deployment-595696685f to 1
  Normal  DeploymentRollback  15m                deployment-controller  Rolled back deployment "nginx-deployment" to revision 2
  Normal  ScalingReplicaSet   15m                deployment-controller  Scaled down replica set nginx-deployment-595696685f to 0
  Normal  ScalingReplicaSet   6m (x16 over 12m)  deployment-controller  (combined from similar events): Scaled down replica set nginx-deployment-c4747d96c to 0

More Detail:
$ kubectl get deployment nginx-deployment -o yaml
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  annotations:
    deployment.kubernetes.io/revision: "5"
  creationTimestamp: 2018-07-16T23:43:16Z
  generation: 8
  labels:
    app: nginx
  name: nginx-deployment
  namespace: default
  resourceVersion: "26304"
  selfLink: /apis/extensions/v1beta1/namespaces/default/deployments/nginx-deployment
  uid: 02c7d8ff-8952-11e8-ac74-0203bdfaca56
spec:
  progressDeadlineSeconds: 600
  replicas: 10
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app: nginx
  strategy:
    rollingUpdate:
      maxSurge: 25%
      maxUnavailable: 25%
    type: RollingUpdate
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: nginx
    spec:
      containers:
      - image: nginx:1.8
        imagePullPolicy: IfNotPresent
        name: nginx
        ports:
        - containerPort: 80
          protocol: TCP
        resources: {}
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
      dnsPolicy: ClusterFirst
      restartPolicy: Always
      schedulerName: default-scheduler
      securityContext: {}
      terminationGracePeriodSeconds: 30
status:
  availableReplicas: 10
  conditions:
  - lastTransitionTime: 2018-07-17T00:03:40Z
    lastUpdateTime: 2018-07-17T00:03:40Z
    message: Deployment has minimum availability.
    reason: MinimumReplicasAvailable
    status: "True"
    type: Available
  - lastTransitionTime: 2018-07-17T00:09:29Z
    lastUpdateTime: 2018-07-17T00:09:43Z
    message: ReplicaSet "nginx-deployment-6ddf4c5bf7" has successfully progressed.
    reason: NewReplicaSetAvailable
    status: "True"
    type: Progressing
  observedGeneration: 8
  readyReplicas: 10
  replicas: 10
  updatedReplicas: 10
```

Delete nginx deployment
```
$ kubectl delete deployment nginx-deployment
deployment.extensions "nginx-deployment" deleted
```

## Understand the primitives necessary to create a self-healing application
Reference from kubernetes.io - [Configure Liveness and Readiness Probes](https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/)

The kubelet uses liveness probes to know when to restart a Container. For example, liveness probes could catch a deadlock, where an application is running, but unable to make progress. Restarting a Container in such a state can help to make the application more available despite bugs.

The kubelet uses readiness probes to know when a Container is ready to start accepting traffic. A Pod is considered ready when all of its Containers are ready. One use of this signal is to control which Pods are used as backends for Services. When a Pod is not ready, it is removed from Service load balancers.

### Define a liveness command
Here is a Pod definition that runs a liveness container based off of a busybox image. For our example we will call this `liveness.yaml`:
```
apiVersion: v1
kind: Pod
metadata:
  labels:
    test: liveness
  name: liveness-exec
spec:
  containers:
  - name: liveness
    image: k8s.gcr.io/busybox
    args:
    - /bin/sh
    - -c
    - touch /tmp/healthy; sleep 30; rm -rf /tmp/healthy; sleep 600
    livenessProbe:
      exec:
        command:
        - cat
        - /tmp/healthy
      initialDelaySeconds: 5
      periodSeconds: 5
```

In the configuration file, you can see that the Pod has a single Container. The periodSeconds field specifies that the kubelet should perform a liveness probe every 5 seconds. The initialDelaySeconds field tells the kubelet that it should wait 5 second before performing the first probe. To perform a probe, the kubelet executes the command cat /tmp/healthy in the Container. If the command succeeds, it returns 0, and the kubelet considers the Container to be alive and healthy. If the command returns a non-zero value, the kubelet kills the Container and restarts it.

When the Container starts, it executes this command: `/bin/sh -c "touch /tmp/healthy; sleep 30; rm -rf /tmp/healthy; sleep 600"`

For the first 30 seconds of the Container’s life, there is a /tmp/healthy file. So during the first 30 seconds, the command cat /tmp/healthy returns a success code. After 30 seconds, cat /tmp/healthy returns a failure code.

Create the Pod and check Pod events
```
$ kubectl create -f liveness.yaml
pod "liveness-exec" created

$ kubectl get pods
NAME            READY     STATUS    RESTARTS   AGE
liveness-exec   1/1       Running   1          1m

$ kubectl describe pod liveness-exec
<...>
Events:
  Type     Reason                 Age               From                                           Message
  ----     ------                 ----              ----                                           -------
  Normal   Scheduled              2m                default-scheduler                              Successfully assigned liveness-exec to kube-node-1-kubelet.kubernetes.mesos
  Normal   SuccessfulMountVolume  2m                kubelet, kube-node-1-kubelet.kubernetes.mesos  MountVolume.SetUp succeeded for volume "default-token-lcvwk"
  Warning  Unhealthy              49s (x6 over 2m)  kubelet, kube-node-1-kubelet.kubernetes.mesos  Liveness probe failed: cat: can't open '/tmp/healthy': No such file or directory
  Normal   Pulling                19s (x3 over 2m)  kubelet, kube-node-1-kubelet.kubernetes.mesos  pulling image "k8s.gcr.io/busybox"
  Normal   Killing                19s (x2 over 1m)  kubelet, kube-node-1-kubelet.kubernetes.mesos  Killing container with id docker://liveness:Container failed liveness probe.. Container will be killed and recreated.
  Normal   Pulled                 18s (x3 over 2m)  kubelet, kube-node-1-kubelet.kubernetes.mesos  Successfully pulled image "k8s.gcr.io/busybox"
  Normal   Created                18s (x3 over 2m)  kubelet, kube-node-1-kubelet.kubernetes.mesos  Created container
  Normal   Started                18s (x3 over 2m)  kubelet, kube-node-1-kubelet.kubernetes.mesos  Started container

$ kubectl get pod liveness-exec
NAME            READY     STATUS    RESTARTS   AGE
liveness-exec   1/1       Running   4          5m
```

You can see in the example above that the container was restarted because the Liveness probe failed, and then was restarted and successfully deployed. The `RESTARTS` field in the output of `kubectl get pod liveness-exec` also shows that the Container was restarted several times before it was successfully deployed.

### Define a liveness HTTP request
Another kind of liveness probe uses an HTTP GET request. Here is the configuration file for a Pod that runs a container based on the k8s.gcr.io/liveness image. For this example we will name this file `http-liveness.yaml`
```
apiVersion: v1
kind: Pod
metadata:
  labels:
    test: liveness
  name: liveness-http
spec:
  containers:
  - name: liveness
    image: k8s.gcr.io/liveness
    args:
    - /server
    livenessProbe:
      httpGet:
        path: /healthz
        port: 8080
        httpHeaders:
        - name: X-Custom-Header
          value: Awesome
      initialDelaySeconds: 3
      periodSeconds: 3
```

In the configuration file, you can see that the Pod has a single Container. The periodSeconds field specifies that the kubelet should perform a liveness probe every 3 seconds. The initialDelaySeconds field tells the kubelet that it should wait 3 seconds before performing the first probe. To perform a probe, the kubelet sends an HTTP GET request to the server that is running in the Container and listening on port 8080. If the handler for the server’s /healthz path returns a success code, the kubelet considers the Container to be alive and healthy. If the handler returns a failure code, the kubelet kills the Container and restarts it.

Any code greater than or equal to 200 and less than 400 indicates success. Any other code indicates failure.

Deploy the `http-liveness.yaml` and describe the Pod:
```
$ kubectl create -f http-liveness.yaml
pod "liveness-http" created

$ kubectl describe pod liveness-http
<...>
Events:
  Type     Reason                 Age               From                                           Message
  ----     ------                 ----              ----                                           -------
  Normal   Scheduled              1m                default-scheduler                              Successfully assigned liveness-http to kube-node-1-kubelet.kubernetes.mesos
  Normal   SuccessfulMountVolume  1m                kubelet, kube-node-1-kubelet.kubernetes.mesos  MountVolume.SetUp succeeded for volume "default-token-lcvwk"
  Normal   Pulling                26s (x3 over 1m)  kubelet, kube-node-1-kubelet.kubernetes.mesos  pulling image "k8s.gcr.io/liveness"
  Normal   Pulled                 25s (x3 over 1m)  kubelet, kube-node-1-kubelet.kubernetes.mesos  Successfully pulled image "k8s.gcr.io/liveness"
  Normal   Created                25s (x3 over 1m)  kubelet, kube-node-1-kubelet.kubernetes.mesos  Created container
  Normal   Started                25s (x3 over 1m)  kubelet, kube-node-1-kubelet.kubernetes.mesos  Started container
  Warning  Unhealthy              8s (x9 over 50s)  kubelet, kube-node-1-kubelet.kubernetes.mesos  Liveness probe failed: HTTP probe failed with statuscode: 500
  Normal   Killing                8s (x3 over 44s)  kubelet, kube-node-1-kubelet.kubernetes.mesos  Killing container with id docker://liveness:Container failed liveness probe.. Container will be killed and recreated.

$ kubectl delete pods liveness-http
pod "liveness-http" deleted
```

### Define a TCP liveness probe
A third type of liveness probe uses a TCP Socket. With this configuration, the kubelet will attempt to open a socket to your container on the specified port. If it can establish a connection, the container is considered healthy, if it can’t it is considered a failure.

For this example we will name this file `tcp-liveness.yaml`
```
apiVersion: v1
kind: Pod
metadata:
  name: goproxy
  labels:
    app: goproxy
spec:
  containers:
  - name: goproxy
    image: k8s.gcr.io/goproxy:0.1
    ports:
    - containerPort: 8080
    readinessProbe:
      tcpSocket:
        port: 8080
      initialDelaySeconds: 5
      periodSeconds: 10
    livenessProbe:
      tcpSocket:
        port: 8080
      initialDelaySeconds: 15
      periodSeconds: 20
```

As you can see, configuration for a TCP check is quite similar to an HTTP check. This example uses both readiness and liveness probes. The kubelet will send the first readiness probe 5 seconds after the container starts. This will attempt to connect to the goproxy container on port 8080. If the probe succeeds, the pod will be marked as ready. The kubelet will continue to run this check every 10 seconds.

In addition to the readiness probe, this configuration includes a liveness probe. The kubelet will run the first liveness probe 15 seconds after the container starts. Just like the readiness probe, this will attempt to connect to the goproxy container on port 8080. If the liveness probe fails, the container will be restarted.


Deploy the `tcp-liveness.yaml` check and describe the Pod: 
```
$ kubectl create -f tcp-liveness.yaml
pod "goproxy" created

$ kubectl get pods
NAME      READY     STATUS    RESTARTS   AGE
goproxy   1/1       Running   0          1m

$ kubectl describe pod goproxy
Events:
  Type    Reason                 Age   From                                           Message
  ----    ------                 ----  ----                                           -------
  Normal  Scheduled              1m    default-scheduler                              Successfully assigned goproxy to kube-node-1-kubelet.kubernetes.mesos
  Normal  SuccessfulMountVolume  1m    kubelet, kube-node-1-kubelet.kubernetes.mesos  MountVolume.SetUp succeeded for volume "default-token-lcvwk"
  Normal  Pulling                1m    kubelet, kube-node-1-kubelet.kubernetes.mesos  pulling image "k8s.gcr.io/goproxy:0.1"
  Normal  Pulled                 1m    kubelet, kube-node-1-kubelet.kubernetes.mesos  Successfully pulled image "k8s.gcr.io/goproxy:0.1"
  Normal  Created                1m    kubelet, kube-node-1-kubelet.kubernetes.mesos  Created container
  Normal  Started                1m    kubelet, kube-node-1-kubelet.kubernetes.mesos  Started container

$ kubectl delete pod goproxy
pod "goproxy" deleted
```

### Define readiness probes
Sometimes, applications are temporarily unable to serve traffic. For example, an application might need to load large data or configuration files during startup. In such cases, you don’t want to kill the application, but you don’t want to send it requests either. Kubernetes provides readiness probes to detect and mitigate these situations. A pod with containers reporting that they are not ready does not receive traffic through Kubernetes Services.

Readiness probes are configured similarly to liveness probes. The only difference is that you use the readinessProbe field instead of the livenessProbe field. Configuration for HTTP and TCP readiness probes also remains identical to liveness probes. Readiness and liveness probes can be used in parallel for the same container. Using both can ensure that traffic does not reach a container that is not ready for it, and that containers are restarted when they fail.

```
readinessProbe:
  exec:
    command:
    - cat
    - /tmp/healthy
  initialDelaySeconds: 5
  periodSeconds: 5
```



